/**
 * Arweave Database Backup System
 *
 * This module provides production-ready automated backups of PostgreSQL database
 * to Arweave for permanent, censorship-resistant storage.
 *
 * Features:
 * - Automated daily/weekly/monthly backups
 * - Incremental and full backup support
 * - Compression (gzip) before upload
 * - Encryption support (optional)
 * - Backup verification and restore
 * - Cost tracking for backup storage
 */

import { TurboFactory, ArweaveSigner } from '@ardrive/turbo-sdk/node';
import { Keypair } from '@solana/web3.js';
import { exec } from 'child_process';
import { promisify } from 'util';
import fs from 'fs';
import path from 'path';
import crypto from 'crypto';
import { createGzip, createGunzip } from 'zlib';
import { pipeline } from 'stream/promises';

const execAsync = promisify(exec);

/**
 * Backup configuration
 */
export interface BackupConfig {
  // Database connection
  dbHost: string;
  dbPort: number;
  dbName: string;
  dbUser: string;
  dbPassword: string;

  // Backup settings
  backupType: 'full' | 'incremental' | 'schema-only' | 'data-only';
  compressionEnabled: boolean;
  encryptionEnabled: boolean;
  encryptionKey?: string; // AES-256 encryption key (32 bytes hex)

  // Arweave settings
  turboWallet: Keypair;
  backupTags: { name: string; value: string }[];

  // Retention
  retentionDays?: number; // Not enforced on Arweave (permanent), but tracked in metadata
}

/**
 * Backup result
 */
export interface BackupResult {
  backupId: string;
  txId: string; // Arweave transaction ID
  arweaveUrl: string;
  backupType: string;
  fileSizeBytes: number;
  compressedSizeBytes: number;
  encrypted: boolean;
  costWinc: string;
  costSol: number;
  costUsd: number;
  timestamp: number;
  checksumSha256: string;
  restorable: boolean;
}

/**
 * Backup metadata (stored in database for tracking)
 */
export interface BackupMetadata {
  backup_id: string;
  tx_id: string;
  arweave_url: string;
  backup_type: string;
  file_size_bytes: number;
  compressed_size_bytes: number;
  encrypted: boolean;
  encryption_algorithm?: string;
  checksum_sha256: string;
  cost_winc: string;
  cost_sol: number;
  cost_usd: number;
  created_at: Date;
  expires_at?: Date; // Soft expiry (for metadata cleanup, not actual deletion)
  restorable: boolean;
  restore_tested_at?: Date;
}

/**
 * Default configuration
 */
const DEFAULT_BACKUP_DIR = './backups';
const DEFAULT_RETENTION_DAYS = 90; // Track backups for 90 days in metadata

/**
 * Generate unique backup ID
 */
function generateBackupId(backupType: string): string {
  const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
  return `db-backup-${backupType}-${timestamp}`;
}

/**
 * Calculate SHA-256 checksum of a file
 */
async function calculateChecksum(filePath: string): Promise<string> {
  return new Promise((resolve, reject) => {
    const hash = crypto.createHash('sha256');
    const stream = fs.createReadStream(filePath);

    stream.on('data', (data) => hash.update(data));
    stream.on('end', () => resolve(hash.digest('hex')));
    stream.on('error', reject);
  });
}

/**
 * Create PostgreSQL database dump
 *
 * @param config - Backup configuration
 * @param outputPath - Path to save dump file
 * @returns Path to created dump file
 */
export async function createDatabaseDump(
  config: BackupConfig,
  outputPath: string
): Promise<string> {
  console.log(`\nüì¶ Creating database dump: ${config.backupType}`);

  // Build pg_dump command
  let pgDumpCmd = `PGPASSWORD="${config.dbPassword}" pg_dump`;
  pgDumpCmd += ` -h ${config.dbHost}`;
  pgDumpCmd += ` -p ${config.dbPort}`;
  pgDumpCmd += ` -U ${config.dbUser}`;
  pgDumpCmd += ` -d ${config.dbName}`;
  pgDumpCmd += ` -F c`; // Custom format (compressed, supports parallel restore)

  // Backup type options
  switch (config.backupType) {
    case 'schema-only':
      pgDumpCmd += ` --schema-only`;
      break;
    case 'data-only':
      pgDumpCmd += ` --data-only`;
      break;
    case 'incremental':
      // Note: True incremental backups require WAL archiving
      // This is a simplified version (full backup with recent data)
      pgDumpCmd += ` --exclude-table-data='archived_*'`; // Example: exclude archived tables
      break;
  }

  pgDumpCmd += ` -f ${outputPath}`;

  try {
    const { stdout, stderr } = await execAsync(pgDumpCmd);

    if (stderr && !stderr.includes('WARNING')) {
      console.warn(`‚ö†Ô∏è  pg_dump warnings: ${stderr}`);
    }

    console.log(`‚úÖ Database dump created: ${outputPath}`);
    return outputPath;
  } catch (error) {
    throw new Error(`Database dump failed: ${error}`);
  }
}

/**
 * Compress file using gzip
 *
 * @param inputPath - Path to input file
 * @param outputPath - Path to output compressed file
 * @returns Path to compressed file
 */
export async function compressFile(
  inputPath: string,
  outputPath: string
): Promise<string> {
  console.log(`üóúÔ∏è  Compressing: ${path.basename(inputPath)}`);

  await pipeline(
    fs.createReadStream(inputPath),
    createGzip({ level: 9 }), // Maximum compression
    fs.createWriteStream(outputPath)
  );

  const originalSize = fs.statSync(inputPath).size;
  const compressedSize = fs.statSync(outputPath).size;
  const ratio = ((1 - compressedSize / originalSize) * 100).toFixed(1);

  console.log(`‚úÖ Compressed: ${originalSize} ‚Üí ${compressedSize} bytes (${ratio}% reduction)`);
  return outputPath;
}

/**
 * Encrypt file using AES-256-CBC
 *
 * @param inputPath - Path to input file
 * @param outputPath - Path to output encrypted file
 * @param encryptionKey - 32-byte encryption key (hex string)
 * @returns Path to encrypted file
 */
export async function encryptFile(
  inputPath: string,
  outputPath: string,
  encryptionKey: string
): Promise<string> {
  console.log(`üîí Encrypting: ${path.basename(inputPath)}`);

  const key = Buffer.from(encryptionKey, 'hex');
  const iv = crypto.randomBytes(16); // Initialization vector

  const cipher = crypto.createCipheriv('aes-256-cbc', key, iv);

  // Prepend IV to encrypted file (needed for decryption)
  const output = fs.createWriteStream(outputPath);
  output.write(iv);

  await pipeline(
    fs.createReadStream(inputPath),
    cipher,
    output
  );

  console.log(`‚úÖ Encrypted: ${outputPath}`);
  return outputPath;
}

/**
 * Decrypt file using AES-256-CBC
 *
 * @param inputPath - Path to encrypted file
 * @param outputPath - Path to output decrypted file
 * @param encryptionKey - 32-byte encryption key (hex string)
 * @returns Path to decrypted file
 */
export async function decryptFile(
  inputPath: string,
  outputPath: string,
  encryptionKey: string
): Promise<string> {
  console.log(`üîì Decrypting: ${path.basename(inputPath)}`);

  const key = Buffer.from(encryptionKey, 'hex');

  // Read IV from first 16 bytes
  const input = fs.createReadStream(inputPath);
  const iv = await new Promise<Buffer>((resolve) => {
    input.once('readable', () => {
      resolve(input.read(16));
    });
  });

  const decipher = crypto.createDecipheriv('aes-256-cbc', key, iv);

  await pipeline(
    input,
    decipher,
    fs.createWriteStream(outputPath)
  );

  console.log(`‚úÖ Decrypted: ${outputPath}`);
  return outputPath;
}

/**
 * Upload backup to Arweave
 *
 * @param turbo - Authenticated Turbo client
 * @param filePath - Path to backup file
 * @param config - Backup configuration
 * @returns Upload result
 */
export async function uploadBackupToArweave(
  turbo: any,
  filePath: string,
  config: BackupConfig,
  backupId: string
): Promise<{ txId: string; costWinc: string }> {
  console.log(`\n‚òÅÔ∏è  Uploading to Arweave: ${path.basename(filePath)}`);

  const stats = fs.statSync(filePath);
  const sizeBytes = stats.size;

  // Estimate cost
  const [{ winc: estimatedCost }] = await turbo.getUploadCosts({ bytes: [sizeBytes] });
  console.log(`üí∞ Estimated cost: ${estimatedCost} winc (~${(parseInt(estimatedCost) / 9_000_000 * 0.000012).toFixed(6)} SOL)`);

  // Upload with tags
  const tags = [
    { name: 'Content-Type', value: 'application/octet-stream' },
    { name: 'App-Name', value: 'SlopMachine' },
    { name: 'App-Version', value: '2.0' },
    { name: 'Backup-ID', value: backupId },
    { name: 'Backup-Type', value: config.backupType },
    { name: 'Database-Name', value: config.dbName },
    { name: 'Compressed', value: config.compressionEnabled.toString() },
    { name: 'Encrypted', value: config.encryptionEnabled.toString() },
    { name: 'Timestamp', value: Date.now().toString() },
    ...config.backupTags,
  ];

  const { id: txId } = await turbo.uploadFile({
    fileStreamFactory: () => fs.createReadStream(filePath),
    fileSizeFactory: () => sizeBytes,
    dataItemOpts: { tags },
  });

  const url = `https://arweave.net/${txId}`;
  console.log(`‚úÖ Uploaded to Arweave: ${url}`);

  return { txId, costWinc: estimatedCost };
}

/**
 * Complete backup workflow
 *
 * @param config - Backup configuration
 * @returns Backup result
 */
export async function performBackup(config: BackupConfig): Promise<BackupResult> {
  console.log('=== Arweave Database Backup ===');
  console.log(`Backup Type: ${config.backupType}`);
  console.log(`Database: ${config.dbName}`);
  console.log(`Compression: ${config.compressionEnabled ? 'Enabled' : 'Disabled'}`);
  console.log(`Encryption: ${config.encryptionEnabled ? 'Enabled' : 'Disabled'}`);

  // Create backup directory
  if (!fs.existsSync(DEFAULT_BACKUP_DIR)) {
    fs.mkdirSync(DEFAULT_BACKUP_DIR, { recursive: true });
  }

  // Generate backup ID
  const backupId = generateBackupId(config.backupType);

  // Step 1: Create database dump
  const dumpPath = path.join(DEFAULT_BACKUP_DIR, `${backupId}.dump`);
  await createDatabaseDump(config, dumpPath);
  const originalSize = fs.statSync(dumpPath).size;

  let processingPath = dumpPath;
  let compressedSize = originalSize;

  // Step 2: Compress (optional)
  if (config.compressionEnabled) {
    const compressedPath = `${dumpPath}.gz`;
    await compressFile(dumpPath, compressedPath);
    compressedSize = fs.statSync(compressedPath).size;
    processingPath = compressedPath;

    // Delete uncompressed dump
    fs.unlinkSync(dumpPath);
  }

  // Step 3: Encrypt (optional)
  if (config.encryptionEnabled) {
    if (!config.encryptionKey) {
      throw new Error('Encryption enabled but no encryption key provided');
    }

    const encryptedPath = `${processingPath}.enc`;
    await encryptFile(processingPath, encryptedPath, config.encryptionKey);

    // Delete unencrypted file
    fs.unlinkSync(processingPath);
    processingPath = encryptedPath;
  }

  // Step 4: Calculate checksum
  const checksum = await calculateChecksum(processingPath);
  console.log(`‚úÖ Checksum (SHA-256): ${checksum}`);

  // Step 5: Upload to Arweave
  const turbo = TurboFactory.authenticated({
    signer: new ArweaveSigner(config.turboWallet.secretKey),
  });

  const { txId, costWinc } = await uploadBackupToArweave(
    turbo,
    processingPath,
    config,
    backupId
  );

  // Step 6: Calculate costs
  const costSol = parseInt(costWinc) / 9_000_000 * 0.000012;
  const costUsd = costSol * 200; // Assume SOL = $200 (use live price in production)

  // Step 7: Clean up local file (optional - keep for verification)
  // fs.unlinkSync(processingPath);

  const result: BackupResult = {
    backupId,
    txId,
    arweaveUrl: `https://arweave.net/${txId}`,
    backupType: config.backupType,
    fileSizeBytes: originalSize,
    compressedSizeBytes: compressedSize,
    encrypted: config.encryptionEnabled,
    costWinc,
    costSol,
    costUsd,
    timestamp: Date.now(),
    checksumSha256: checksum,
    restorable: true,
  };

  console.log('\n‚úÖ Backup completed successfully!');
  console.log(`   Backup ID: ${backupId}`);
  console.log(`   Arweave URL: ${result.arweaveUrl}`);
  console.log(`   Cost: ${costSol.toFixed(6)} SOL (~$${costUsd.toFixed(4)} USD)`);

  return result;
}

/**
 * Download backup from Arweave
 *
 * @param txId - Arweave transaction ID
 * @param outputPath - Path to save downloaded file
 * @returns Path to downloaded file
 */
export async function downloadBackupFromArweave(
  txId: string,
  outputPath: string
): Promise<string> {
  console.log(`\n‚¨áÔ∏è  Downloading backup from Arweave: ${txId}`);

  const url = `https://arweave.net/${txId}`;
  const response = await fetch(url);

  if (!response.ok) {
    throw new Error(`Failed to download backup: ${response.statusText}`);
  }

  const buffer = await response.arrayBuffer();
  fs.writeFileSync(outputPath, Buffer.from(buffer));

  console.log(`‚úÖ Downloaded: ${outputPath} (${buffer.byteLength} bytes)`);
  return outputPath;
}

/**
 * Restore database from backup
 *
 * @param backupPath - Path to backup file (decrypted, decompressed)
 * @param config - Backup configuration
 */
export async function restoreDatabase(
  backupPath: string,
  config: BackupConfig
): Promise<void> {
  console.log(`\nüîÑ Restoring database from backup: ${backupPath}`);

  // Build pg_restore command
  let pgRestoreCmd = `PGPASSWORD="${config.dbPassword}" pg_restore`;
  pgRestoreCmd += ` -h ${config.dbHost}`;
  pgRestoreCmd += ` -p ${config.dbPort}`;
  pgRestoreCmd += ` -U ${config.dbUser}`;
  pgRestoreCmd += ` -d ${config.dbName}`;
  pgRestoreCmd += ` --clean`; // Drop existing objects before restoring
  pgRestoreCmd += ` --if-exists`; // Don't error if objects don't exist
  pgRestoreCmd += ` -j 4`; // Parallel restore (4 jobs)
  pgRestoreCmd += ` ${backupPath}`;

  try {
    const { stdout, stderr } = await execAsync(pgRestoreCmd);

    if (stderr && !stderr.includes('WARNING')) {
      console.warn(`‚ö†Ô∏è  pg_restore warnings: ${stderr}`);
    }

    console.log(`‚úÖ Database restored successfully`);
  } catch (error) {
    throw new Error(`Database restore failed: ${error}`);
  }
}

/**
 * Complete restore workflow
 *
 * @param txId - Arweave transaction ID
 * @param config - Backup configuration
 */
export async function performRestore(
  txId: string,
  config: BackupConfig
): Promise<void> {
  console.log('=== Arweave Database Restore ===');
  console.log(`Transaction ID: ${txId}`);

  // Create restore directory
  const restoreDir = path.join(DEFAULT_BACKUP_DIR, 'restore');
  if (!fs.existsSync(restoreDir)) {
    fs.mkdirSync(restoreDir, { recursive: true });
  }

  // Step 1: Download from Arweave
  const downloadPath = path.join(restoreDir, `backup-${txId}.enc`);
  await downloadBackupFromArweave(txId, downloadPath);

  let processingPath = downloadPath;

  // Step 2: Decrypt (if encrypted)
  if (config.encryptionEnabled) {
    if (!config.encryptionKey) {
      throw new Error('Backup is encrypted but no encryption key provided');
    }

    const decryptedPath = path.join(restoreDir, `backup-${txId}.gz`);
    await decryptFile(processingPath, decryptedPath, config.encryptionKey);
    processingPath = decryptedPath;
  }

  // Step 3: Decompress (if compressed)
  if (config.compressionEnabled) {
    const decompressedPath = path.join(restoreDir, `backup-${txId}.dump`);

    await pipeline(
      fs.createReadStream(processingPath),
      createGunzip(),
      fs.createWriteStream(decompressedPath)
    );

    console.log(`‚úÖ Decompressed: ${decompressedPath}`);
    processingPath = decompressedPath;
  }

  // Step 4: Restore database
  await restoreDatabase(processingPath, config);

  console.log('\n‚úÖ Restore completed successfully!');
}

/**
 * Automated backup scheduler
 *
 * @param config - Backup configuration
 * @param schedule - Cron-like schedule ('daily', 'weekly', 'monthly')
 */
export async function scheduleBackups(
  config: BackupConfig,
  schedule: 'daily' | 'weekly' | 'monthly'
): Promise<void> {
  console.log(`üìÖ Scheduling ${schedule} backups`);

  const intervals = {
    daily: 24 * 60 * 60 * 1000, // 24 hours
    weekly: 7 * 24 * 60 * 60 * 1000, // 7 days
    monthly: 30 * 24 * 60 * 60 * 1000, // 30 days
  };

  const intervalMs = intervals[schedule];

  // Perform initial backup
  await performBackup(config);

  // Schedule recurring backups
  setInterval(async () => {
    console.log(`\nüîî Scheduled ${schedule} backup triggered`);
    await performBackup(config);
  }, intervalMs);
}

/**
 * Example: Daily automated backups
 */
export async function exampleAutomatedBackup() {
  const config: BackupConfig = {
    dbHost: process.env.DB_HOST || 'localhost',
    dbPort: parseInt(process.env.DB_PORT || '5432'),
    dbName: process.env.DB_NAME || 'slopmachine',
    dbUser: process.env.DB_USER || 'postgres',
    dbPassword: process.env.DB_PASSWORD || '',

    backupType: 'full',
    compressionEnabled: true,
    encryptionEnabled: true,
    encryptionKey: process.env.BACKUP_ENCRYPTION_KEY, // 32-byte hex key

    turboWallet: Keypair.fromSecretKey(
      Buffer.from(JSON.parse(process.env.BACKUP_WALLET || '[]'))
    ),

    backupTags: [
      { name: 'Environment', value: 'production' },
      { name: 'Retention-Days', value: '90' },
    ],

    retentionDays: 90,
  };

  // Perform single backup
  const result = await performBackup(config);

  console.log('\nüìä Backup stored in database:');
  console.log(JSON.stringify(result, null, 2));

  // Store metadata in database
  // await db.insertBackupMetadata(result);
}

// Run example if executed directly
if (require.main === module) {
  exampleAutomatedBackup().catch(console.error);
}
